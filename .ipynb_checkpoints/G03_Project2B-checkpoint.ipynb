{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pràctica: Project 2B  \n",
    "Autores: Bo Miquel Nordfeldt, Joan Muntaner, Helena Antich  \n",
    "Fecha: Abril 2021 \n",
    "# Descripción de la práctica  \n",
    "Breve descripción de la práctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código inicial  \n",
    "Descripción del código inicial dado por el curso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.6815 - accuracy: 0.5670 - val_loss: 0.6655 - val_accuracy: 0.5993\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.6312 - accuracy: 0.6436 - val_loss: 0.5645 - val_accuracy: 0.6545\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.5730 - accuracy: 0.7015 - val_loss: 0.5986 - val_accuracy: 0.7068\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.5440 - accuracy: 0.7240 - val_loss: 0.4699 - val_accuracy: 0.7576\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.5151 - accuracy: 0.7460 - val_loss: 0.5700 - val_accuracy: 0.7627\n",
      "Epoch 6/25\n",
      "207/250 [=======================>......] - ETA: 3s - loss: 0.4945 - accuracy: 0.7533"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3cab7e045c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m                                    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                                    validation_steps = 62)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\base1\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'C:\\Users\\munta\\OneDrive\\Escritorio\\Aprendizaje Profundo\\G03_Project2B\\G03_Project2B\\deeplearning-az-master\\datasets\\Part 2 - Convolutional Neural Networks (CNN)\\dataset\\training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'C:\\Users\\munta\\OneDrive\\Escritorio\\Aprendizaje Profundo\\G03_Project2B\\G03_Project2B\\deeplearning-az-master\\datasets\\Part 2 - Convolutional Neural Networks (CNN)\\dataset\\test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "history = classifier.fit_generator(training_set,\n",
    "                                   steps_per_epoch = 250,\n",
    "                                   epochs = 25,\n",
    "                                   validation_data = test_set,\n",
    "                                   validation_steps = 62)\n",
    "\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "\n",
    "# Part 3 - Making new predictions\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deficiencias del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas realizadas  \n",
    "Se han realizado diversas pruebas, se pueden encontrar algunas en la carpeta adjunta de \"pruebas\". Dos de las más destacadas son: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba ejemplo 1  \n",
    "Descripción de la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías y paquetes\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parte 1 - Construir el modelo de CNN\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=20,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba ejemplo 2  \n",
    "Descripción de la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Parte 1 - Construir el modelo de CNN\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "#classifier.add(Flatten())\n",
    "\n",
    "#################################################################\n",
    "\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "################################################################################\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=60,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN final\n",
    "Después de las diversas pruebas realizadas se ha llegado a que...\n",
    "\n",
    "## Título bloque siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "#classifier.add(Flatten())\n",
    "\n",
    "#################################################################\n",
    "\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción  \n",
    "\n",
    "## Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrip\n",
    "\n",
    "## Compilar CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrip  \n",
    "\n",
    "## Título bloque siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=150,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrip  \n",
    "\n",
    "## Plot de la evolución de la pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrip  \n",
    "\n",
    "## Comprobación de la funcionalidad de la CNN\n",
    "\n",
    "Descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
