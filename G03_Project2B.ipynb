{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd038b128c53ff5e69cf4a6187eab66f2cd5adf3eac10fabfcc2b709910ad8ad29c",
   "display_name": "Python 3.7.7 64-bit ('helena2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Pràctica: Project 2B  \n",
    "Autores: Bo Miquel Nordfeldt, Joan Muntaner, Helena Antich  \n",
    "Fecha de entrega: 29/04/2021  \n",
    "# Descripción de la práctica  \n",
    "Breve descripción de la práctica:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Código inicial  \n",
    "Descripción del código inicial dado por el curso\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/bomiquel/MUSI/ItinerariInteligenciaArtificial/AprendizajeProfundo/Proyectos/G03_Project2B/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/bomiquel/MUSI/ItinerariInteligenciaArtificial/AprendizajeProfundo/Proyectos/G03_Project2B/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "history = classifier.fit_generator(training_set,\n",
    "                                   steps_per_epoch = 250,\n",
    "                                   epochs = 25,\n",
    "                                   validation_data = test_set,\n",
    "                                   validation_steps = 62)\n",
    "\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part 3 - Making new predictions\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "source": [
    "Deficiencias del código"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Pruebas realizadas  \n",
    "Se han realizado diversas pruebas, se pueden encontrar algunas en la carpeta adjunta de \"pruebas\". Dos de las más destacadas son: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prueba ejemplo 1  \n",
    "Descripción de la prueba"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías y paquetes\n",
    "from keras.layers import  Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Parte 1 - Construir el modelo de CNN\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=20,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "source": [
    "## Prueba ejemplo 2  \n",
    "Descripción de la prueba"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Parte 1 - Construir el modelo de CNN\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "#classifier.add(Flatten())\n",
    "\n",
    "#################################################################\n",
    "\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "################################################################################\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=60,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "source": [
    "# CNN final\n",
    "Después de las diversas pruebas realizadas se ha llegado a que...\n",
    "\n",
    "## Título bloque siguiente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Convolución\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "#classifier.add(Flatten())\n",
    "\n",
    "#################################################################\n",
    "\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "\n",
    "# Paso 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Una segunda capa de convolución y max pooling\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25 )) # NUEVO \n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "################################################################################"
   ]
  },
  {
   "source": [
    "Descripción  \n",
    "\n",
    "## Full connection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
    "#classifier.add(Dense(units = 64, activation = \"relu\")) #NUEVO\n",
    "classifier.add(Dense(units = 32, activation = \"relu\")) #NUEVO\n",
    "#este dropout desactiva el 25% de las conexiones entre las neuronas, lo cual mejora los resultados\n",
    "classifier.add(Dropout(0.25)) # NUEVO \n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "source": [
    "Descrip\n",
    "\n",
    "## Compilar CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"RMSprop\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #NUEVO\n",
    "#classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "source": [
    "Descrip  \n",
    "\n",
    "## Título bloque siguiente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "#NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO #NUEVO\n",
    "# A los 15 valores de coste sin variar significativamente deja de ajustar el modelo\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Ajusta el modelo a más de 1000 iteraciones con el 'earlystopper' y lo asigna al historial\n",
    "history = classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=350,\n",
    "                        epochs=150,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=200,\n",
    "                        callbacks = [earlystopper])\n"
   ]
  },
  {
   "source": [
    "Descrip  \n",
    "\n",
    "## Plot de la evolución de la pérdida"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'bo',label='training loss') \n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "Descrip  \n",
    "\n",
    "## Comprobación de la funcionalidad de la CNN\n",
    "\n",
    "Descrip"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  }
 ]
}